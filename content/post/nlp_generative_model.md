+++
title = "Generative Model for text: An overview of recent advancements"
date = 2018-08-26T14:54:06+08:00
draft = false

# Tags and categories
# For example, use `tags = []` for no tags, or the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []
categories = []
math = true
# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
# Use `caption` to display an image caption.
#   Markdown linking is allowed, e.g. `caption = "[Image credit](http://example.org)"`.
# Set `preview` to `false` to disable the thumbnail in listings.
[header]
image = ""
caption = ""
preview = false

+++
{{% toc %}}

# Introduction
The recent progress of generative adversarial network (GAN) has shown siginificant imporvment of learning latent representation of high-dimensional continuous data such as images and videos in vision domain. Many amazing applications such as video synthesis and image translation are based on the capability of GAN.


With these successful applications in vision domain, it is natural to ask: 
Can we apply similar techniques to language to achieve similar results?

Unfortunately, compared with hundreds of development of GAN and its applications (See the [GAN zoo](https://github.com/hindupuravinash/the-gan-zoo)), only a few progress has been made for the text. In general, it is the discrete structure of text that makes apply GAN to language inappropriate, which is designed for continuous data.  According to [Goodfellow](https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/), the generator learns how to slightly change the synethetic data through the gradient information from the discriminator. Such slight change is only meaningful for continuous data. For example, it is resaonable to change a pixel value from 1 to 1 + 0.001 while it is not for changing "penguin" to  "penguin" + 0.001.

Despite of the difficulty, many methods have been proposed in the last two years to address this issue and some of them have many interesting results. In this article, I will give an overview of these methods, issue about evaluation and their applications.

# Model for text generation
## Improved training of GAN
Perhaps the most basic task to study the capability of generating meaningful text is to generate simple characters with certain pattern. Kusner et al. [1] proposes to relax the output multimodal discrete distribution for each time step $t$ in RNN decoder by using Gumbel-softmax, which is a continuous approximation.

\begin{align}
\label{eq:1}
\boldsymbol{y}_{t} = \textrm{softmax}(\frac{ \boldsymbol{h}_t + \boldsymbol{g}_t}{\tau}) (1),
\end{align}
where $\boldsymbol{h}_t$ is the hidden statet of RNN decoder in $t$ step and  $\boldsymbol{g}_t$ is sample from the Gumbel distribution. $\tau$ is a parameter used to control how the close the continuous approximate distribution to the discrete one. When $\tau \rightarrow 0$, Eq (1) is more like to discrete distribution. When $\tau \rightarrow \infty$, it is more like a uniform distribution. We call this Gumbel-softmax trick. Here are some great tutorial by [Erig Jang](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) and [Gonzalo Mena](https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html) if you want to learn more about it.

The result of using Gumbel-softmax seems help GAN in discrete space. As demonstrated Fig 1 clipped from the original paper, the left plot shows the samples generated by MLE and the right one is generated by proposed method which generate similar pattern shown in training data, especially for 4th, 10th and 17th row.

Gulrajani et al.~\cite{} later proposes to apply gradient penalty to improve the training of Wasserstein GAN~\cite{}. The proposed improvement (WGAN-GP) not only enhances the quality of image generation but open a door to generate more complex character. The author directly train a character based language model by 1D-CNN on Billion Word dataset~cite{} to transform a latent vector into a sequence of 32 one-hot character. During training, the output vector from softmax is directly used as input of discriminaotr. When inference, the argmax operation is applied to each output vector. The figure below shows the sample generated by WGAN-GP (up) and the original GAN (bottom), which shows that the original GAN completely fails while WGAN-GP is able to generate complex pattern of character. Despite the generated text is still not readable, this is the first text generation model trained purely in adversarial way without resorting to MLE pre-training, which is a commonly used techniuqes for training generative model for text.
<img class="plain" src ="/img/nlg_overview_fig2.png" style="border-style: none" hegith="800" width="600">
## Policy gradient
## Variational Autoencoder
## Autoencoder
## Alternative decoding objective

# Evaluation
# Application
## Image to text generation
## (Visual) Dialog generation
## Style tranfer for text
# Final Words


