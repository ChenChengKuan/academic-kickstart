+++
title = "Generative Model for text: An overview of recent advancements"
date = 2018-08-26T14:54:06+08:00
draft = false

# Tags and categories
# For example, use `tags = []` for no tags, or the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []
categories = []
math = true
# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
# Use `caption` to display an image caption.
#   Markdown linking is allowed, e.g. `caption = "[Image credit](http://example.org)"`.
# Set `preview` to `false` to disable the thumbnail in listings.
[header]
image = ""
caption = ""
preview = false

+++
{{% toc %}}
<style>
.reveal section img { background:none; border:none; box-shadow:none; }
</style>

# Introduction
The recent progress of generative adversarial network (GAN) has shown siginificant imporvment of learning latent representation of high-dimensional continuous data such as images and videos in vision domain. Many amazing applications such as video synthesis and image translation are based on the capability of GAN.


With these successful applications in vision domain, it is natural to ask: 
Can we apply similar techniques to language to achieve similar results?

Unfortunately, compared with hundreds of development of GAN and its applications (See the [GAN zoo](https://github.com/hindupuravinash/the-gan-zoo)), only a few progress has been made for the text. In general, it is the discrete structure of text that makes apply GAN to language inappropriate, which is designed for continuous data.  According to [Goodfellow](https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/), the generator learns how to slightly change the synethetic data through the gradient information from the discriminator. Such slight change is only meaningful for continuous data. For example, it is resaonable to change a pixel value from 1 to 1 + 0.001 while it is not for changing "penguin" to  "penguin" + 0.001.

Despite of the difficulty, many methods have been proposed in the last two years to address this issue and some of them have many interesting results. In this article, I will give an overview of these methods, issue about evaluation and their applications.

# Model for text generation
## Improved training of GAN
Perhaps the most basic task to study the capability of generating meaningful text is to generate simple characters with certain pattern. Kusner et al. [1] proposes to relax the output multimodal discrete distribution for each time step $t$ in RNN decoder by using Gumbel-softmax, which is a continuous approximation.

\begin{align}
\label{eq:1}
\boldsymbol{y}_{t} = \textrm{softmax}(\frac{ \boldsymbol{h}_t + \boldsymbol{g}_t}{\tau}) (1),
\end{align}
where $\boldsymbol{h}_t$ is the hidden statet of RNN decoder in $t$ step and  $\boldsymbol{g}_t$ is sample from the Gumbel distribution. $\tau$ is a parameter used to control how the close the continuous approximate distribution to the discrete one. When $\tau \rightarrow 0$, Eq (1) is more like to discrete distribution. When $\tau \rightarrow \infty$, it is more like a uniform distribution. We call this Gumbel-softmax trick. Here are some great tutorial by [Erig Jang](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) and [Gonzalo Mena](https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html) if you want to learn more about it.

The result of using Gumbel-softmax seems help GAN in discrete space. As demonstrated Fig 1 clipped from the original paper, the left plot shows the samples generated by MLE and the right one is generated by proposed method which generate similar pattern shown in training data, especially for 4th, 10th and 17th row.
<figure>
<img src="/img/nlg_overview_fig1.png" height="150" width="300" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Samples generated by MLE (left) and GAN with Gumbel-softmax (right)</figcaption>
</figure>
Gulrajani et al.~\cite{} later proposes to apply gradient penalty to improve the training of Wasserstein GAN~\cite{}. The proposed improvement (WGAN-GP) not only enhances the quality of image generation but open a door to generate more complex character. The author directly train a character based language model by 1D-CNN on Billion Word dataset~cite{} to transform a latent vector into a sequence of 32 one-hot character. During training, the output vector from softmax is directly used as input of discriminaotr. When inference, the argmax operation is applied to each output vector. The figure below shows the sample generated by WGAN-GP (up) and the original GAN (bottom), which shows that the original GAN completely fails to generate meaningful pattern while WGAN-GP is able to generate complex pattern of character. We can observe that WGAN-GP start to learn some simple vocabulary such as "In", "the" and "was".  Despite the generated text is still not readable, this is the first text generation model trained purely in adversarial way without resorting to MLE pre-training, which is a commonly used techniuqes for training generative model for text.
<figure>
<img src="/img/nlg_overview_fig2.png" height="768" width="576" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle"> Samples generated by WGAN-GP (up) and standard GAN (bottom)</figcaption>
</figure>

Several recently proposed approaches have shown further improvement of training GAN in discrete space . Hjelm et al. propose boundary seeking GAN (BEGAN)~\cite{} that uses discrimator optimized for estimating f-divergence. They use policy gradient and importance weight derived from the discriminator to guide the generator. Concurrently, Mroueh et al. propose Sobolev GAN~\cite{} that train GAN with new objective with Sobolev norm. Their results are still cannot generate readable sentences<sup>1</sup>.

But what could we get if we try not to be too harsh to our model and allow it to be pre-trained first on some corpus? 

The textGAN proposed by Zhang et al. answers this question. They initialize the weights of LSTM generator from a pre-trained CNN-LSTM autoencoder (i.e. Using the weights of LSTM part to initialize the generator). Inspired by feature matching techniques used to improve GAN in the early day, they train the generator to synthesized data that match empirical distribution of the real data in the feature space by minimizing maximum mean discrepancy (MMD), which can be understood to minimize the moments of two distributions. If the Gaussian kernel is used (which is the case in the paper), it is shown that minimizing MMD will match moments of all orders of two distributions~\cite{}. Figure below shows the result of textGAN, we can see the generator can output some meaningful sentences. In addition, the trajectory of latent space is more resonable and smooth from one sentence to the othter compared to vanila autoencoder, which suggests textGAN learned better representation of text.
<figure>
<img src="/img/nlg_overview_fig3.png" height="960" width="720" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle"> Samples generated by textGAN (left) and the comparison of latent space transition between textGAN and autoencoder</figcaption>
</figure>

Standard GAN framework is designed for continuous data. Applying it to discrete data directly need to reformulate the learning objective or use pretraining to facilitate the learning. Is there any generative model that naturally fit the discrete input data? I will discuss it in the next section.

## Variational Autoencoder
Unlike GAN, Variational Autoencoder (VAE) can work with both continous and discrete input data directly. For peolpe who are not familiar with VAE, I recommend the tutorial ([here](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) and [here](https://arxiv.org/abs/1601.00670)) written by Jaan Altossar and Blei et al.(if you want to go deeper). Here I just give a brief introduction. Given data $\boldsymbol{x}$, latent variable $\boldsymbol{z}$, encoder parameters $\boldsymbol{\theta}$ and decoder parameters $\boldsymbol{\phi}$, the goal of VAE is to approximate the posterior $p(\boldsymbol{z}|\boldsymbol{x})$ by a familiy of distribution $q_\lambda(\boldsymbol{z}|\boldsymbol{x})$, where $\lambda$ indicates the familiy of distribution. Take Gaussian for example, $\boldsymbol{\lambda} = \(\boldsymbol{\mu}, \boldsymbol{\sigma}\)$. This is achievied by maximizing the evidence lower bound (ELBO). For a single data point $x$, the ELBO is
\begin{align}
\textrm{ELOB}\_i = \mathbb{E}\_{q\_{\theta}(z|x\_i)}(\textrm{log}p\_{\phi}(x\_i|z)) - \textrm{KL}(q\_{\theta}(z|x\_i)||p(z))
\end{align}
The first term can be viewed as how well the model can reconstruct data given the learned representation. The second term can be viewed as a regularization which we hope the learned posterior can be close to prior. Maximizing ELBO will encourage the model learn useful latent representation that explain the data well. Note that we use $q\_{\theta}$ to replace the $q\_{\lambda}$ since we use encoder to inference $\lambda$

Actually, the recent generative model for text is based on VAE proposed by Bowman et al. They propose a RNN-based variational autoencoder which capture the global feautre of sentces (e.g., topic, style) in continous variables. The RNN-encoder inference the $\mu$ and $\sigma$ of Gaussian and a sample $z$ is draw from the posterior for decoder to generate the sentences. The architecutre is shown below and many subsequent works follow similar architecture with some modifications.

<figure>
<img src="/img/nlg_overview_fig4.png" height="960" width="720" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Main architecture proposed by Bowman et al. The code z is only concated to the first hidden state of decoder</figcaption>
</figure>


The core issue of using VAE is _latent variable collapse_, which means the KL term in the ELBO become zero during the optimization. This might seem to be confused at first since zero KL term seems to be beneficial to ELBO as we hope to maximize it. However, if we examine the KL term carefully, zero KL term means that the posterior $q\_{\theta}(z|x\_i)$ is equal to prior $p(z)$ thus posterior is independent from the input data! Latent variable collapse thus preclude us from learning useful latent representation from the input data which is a goal we aim to achieve. This is a common case in language modelling as discussed in~\cite{}. Once this happened, the RNN decoder will completely ignore the latent representation and naively use decoder's capability to achieve the optimal. The difficulty here is how do we maximize the ELBO and prevent the KL term from going to zero at the same. 

Bowman et al. propose KL-annealing and word dropout to alleviate this issue, which increase the weight of KL term during training and randomly replace word token by \<UNK\> to weaken the decoder thus force decoder to rely on global representation $z$ instead of learned language model. However, these techniques cannot solve this issue and VAE trained in this way is slightly worse than language model (RNNLM) in NLL despite it can geenrate more plausible sentences than autoencoder when moving in the latent space. Therefore many this line of research focus on finding better techniques to address latent variable collapse and I will dicuss them later.

Yang et al. hypothesize and validate the contexual capacity of decoder is related to latent variable collapse. They replace original RNN decoder with dilated convolutional neural network as the figure below, which facilitates the control of contextual capacity by changing dilation.

<figure>
<img src="/img/nlg_overview_fig5.png" height="960" width="720" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Architecture proposed by Yang et al. The left encoder-decoder similar to Fig. 4 and the right is the detail inside dilated CNN decoder. Note that the code $z$ is concated to each word embedding</figcaption>
</figure>

The dilated CNN used in the decoder is 1D dilated CNN, which is a way to enlarge the reception field without sacrifying computation cost. By controlling the dilation size in CNN. The author study 4 configurations of decoder with following depth and dilation: [1, 2, 4], [1, 2, 4, 8, 16], [1, 2, 4, 8, 16, 1, 2, 4, 8, 16] and [1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16], which are denoted as SCNN, MCNN, LCNN and VLCNN.
They also compare with vanilla VAE proposed by Bownman et al. In each configuration, they further divide 3 training methods for comparison, which are: language model (LM), VAE (architectures proposed in this paper) and VAE + init that initalize the encoder's weight by language model.
The results are shown in Fig. 6

<figure>
<img src="/img/nlg_overview_fig6.png" height="250" width="400" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">NLL and KL term of all methods on Yahoo dataset. There are three bars in each group which are LM, VAE, VAE+init. The red bar is the propotion of KL term in reconstruction loss</figcaption>
</figure>

The vanilla VAE has worse NLL as KL term goes to zero which agrees with the the negative results found by Bowman et al. For the rest baselines based on CNN with different configurations (i.e. dilation, depth), we can see better improvement for small model (SCNN) over pure LM and the improvement gradually diminish as model size become large (VLCNN). This finding suggests that using dilated CNN as decoder could alleviate latent variable collapse if we carefully choose the decoder contextual capacity. Using pretrained language model also helps for the learning. 

The latent space learned by proposed VAE by setting dimension of $z=2$ on Yahoo and Yelp review dataset is visualized in Fig.7. It's interesting to see different topics are mapped to different regions and the sentiments (i.e. rating from 1-5) of review are horinzontally spread. The author extend the proposed method to conditional text generation. Some examples are presented in Fig. 8.

<figure>
<img src="/img/nlg_overview_fig7.png" height="640" width="480" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Regions of different topic and sentiments mapped in the latent space.</figcaption>
</figure>
<figure>
<img src="/img/nlg_overview_fig8.jpg" height="2400" width="6000" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Left: Text generated conditioned on topic labels. Right: Text generated conditioned on sentiment labels.</figcaption>
</figure>

Concurrently, Semeniuta et al. propose a convolutional-deconvolutional VAE with recurrent models on top of the output of deconlolutional layers (Hybrid VAE) in Fig. 9 for text generation. 
<figure>
<img src="/img/nlg_overview_fig9.png" height="880" width="660" style="background:none; border:none; box-shadow:none; margin=0; padding=0"/>
<figcaption align="middle">Left: The convolutioanl-deconvolutional encoder decoder part of proposed hybrid VAE. Right: Two vairant of recurrent model appended on top of ouput of deconvolutional layer, which are traditional LSTM and ByteNet~\cite{} </figcaption>
</figure>

They further introduce an auxiliary loss $J\_{aux} = -\alpha\mathbb{E}\_{q(\boldsymbol{z}|\boldsymbol{x})}\textrm{log}p\_{\phi}(\boldsymbol{x}|\boldsymbol{z})$ into the optimization of ELBO to force the decoding process rely on the latent representation $\boldsymbol{z}$. $\alpha$ is a parameter to control the penalty of auxiliary loss. 

In their experiments, the compare the decoding performane of proposed method with vanilla VAE under historyless and history setting. In other words, they test the decoding performance of models by using word dropout rate $p$ from 1.0 to certain ratio during training (i.e. Randomly replacing the ground truth with \<UNK\> token during training). They also study the benefit of auxilary loss for decoding performance as the expressive power of decoder become stronger (i.e. deeper layer). The results are shown in Fig 10.


Developing new methods to address the latent variable collapse is still an very active research area. In very recent work~\cite{}. The authors propose semi-amortized inference that initializing variatoinal parameter by amortized inference then applying stochastic variational inference to refine them. Another recent work~\cite introduce skip connections between latent variables $\boldsymbol{z}$ and decoder to enforce the relationship between latent variables and reconstrunction loss. Both methods are justified better than previous method by experiments.

## Autoencoder
## Policy gradient
## Alternative decoding objective

# Evaluation
# Application
## Image to text generation
## (Visual) Dialog generation
## Style tranfer for text
# Final Words


